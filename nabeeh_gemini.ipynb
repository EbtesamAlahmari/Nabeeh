{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHvA6hZtSNUx/6oauspwS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EbtesamAlahmari/Nabeeh/blob/main/nabeeh_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=\"AIzaSyAeQdMgOlf33rJAhH3SojYfrg8es1N_37c\")\n",
        "\n",
        "# Load and inspect your dataset (CSV must be uploaded to Colab first)\n",
        "df = pd.read_csv(\"/content/nabeeh_financial_data.csv\")\n",
        "\n",
        "# Display info and columns to verify the new dataset structure\n",
        "# df.info()\n",
        "# display(df.columns)\n",
        "# display(df.head())\n",
        "\n",
        "\n",
        "#  Create a target column for overspending\n",
        "# df[\"TotalExpenses\"] = df[\"monthly_expense_total\"] # Using monthly_expense_total instead of individual expense columns\n",
        "df[\"Overspending\"] = (df[\"TotalExpenses\"] > 0.8 * df[\"Income\"]).astype(int)\n",
        "\n",
        "\n",
        "# Prepare features and labels\n",
        "X = df[[\"Income\", \"TotalExpenses\", \"Loan\"]] # Using TotalExpenses as a feature\n",
        "y = df[\"Overspending\"]\n",
        "\n",
        "# Split and train model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"ğŸ“Š Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ğŸ§© Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Ask Gemini a financial question with user's data\n",
        "def ask_gemini(user_question, monthly_income, total_expenses, loan): # Updated function signature\n",
        "    total = total_expenses + loan\n",
        "    summary = f\"\"\"\n",
        "    Ø¯Ø®Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {monthly_income} Ø±ÙŠØ§Ù„\n",
        "    Ø§Ù„Ù…ØµØ±ÙˆÙ Ø§Ù„ÙƒÙ„ÙŠ: {total_expenses} Ø±ÙŠØ§Ù„ # Updated variable name\n",
        "    Ø§Ù„Ù‚Ø³Ø· Ø§Ù„Ø´Ù‡Ø±ÙŠ: {loan} Ø±ÙŠØ§Ù„\n",
        "    Ø§Ù„Ù…ØµØ±ÙˆÙ Ø§Ù„ÙƒÙ„ÙŠ: {total} Ø±ÙŠØ§Ù„\n",
        "\n",
        "    Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ³Ø£Ù„: {user_question}\n",
        "    \"\"\"\n",
        "    response = genai.GenerativeModel(\"gemini-1.5-flash\").generate_content(summary)\n",
        "    return response.text\n",
        "\n",
        "# Example question\n",
        "print(\"ğŸ¤– Gemini says:\\n\", ask_gemini(\"Ù‡Ù„ Ø£Ù‚Ø¯Ø± Ø£Ø´ØªØ±ÙŠ Ø¢ÙŠÙÙˆÙ† Ø¨Ù€Ù¥Ù Ù Ù ØŸ\", 8000, 5000, 500)) # Updated example with TotalExpenses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "AfTnnrg3dUhc",
        "outputId": "8c8e52eb-bb87-464a-9e96-cb5c4aa60b9a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.90      0.91       172\n",
            "           1       0.98      0.98      0.98       828\n",
            "\n",
            "    accuracy                           0.97      1000\n",
            "   macro avg       0.95      0.94      0.94      1000\n",
            "weighted avg       0.97      0.97      0.97      1000\n",
            "\n",
            "ğŸ§© Confusion Matrix:\n",
            " [[154  18]\n",
            " [ 14 814]]\n",
            "ğŸ¤– Gemini says:\n",
            " Ù„Ù†Ø¨Ø¯Ø£ Ø¨Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„Ù…ØµØ±ÙˆÙØ§Øª ÙˆØ§Ù„Ø£Ù‚Ø³Ø§Ø·:\n",
            "\n",
            "* **Ø§Ù„Ø¯Ø®Ù„:** 8000 Ø±ÙŠØ§Ù„\n",
            "* **Ø§Ù„Ù…ØµØ±ÙˆÙ Ø§Ù„ÙƒÙ„ÙŠ:** 5500 Ø±ÙŠØ§Ù„ (ØªÙ… ØªØ­Ø¯ÙŠØ« Ù‡Ø°Ø§ Ø§Ù„Ø±Ù‚Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¢Ø®Ø± Ù‚ÙŠÙ…Ø© Ù…ÙØ¹Ø·Ø§Ø©)\n",
            "* **Ø§Ù„Ù‚Ø³Ø· Ø§Ù„Ø´Ù‡Ø±ÙŠ:** 500 Ø±ÙŠØ§Ù„ (Ù‡Ø°Ø§ Ø§Ù„Ø±Ù‚Ù… Ù„ÙŠØ³ Ù„Ù‡ ØªØ£Ø«ÙŠØ± Ù…Ø¨Ø§Ø´Ø± Ø¹Ù„Ù‰ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø´Ø±Ø§Ø¡ Ø§Ù„Ø¢ÙŠÙÙˆÙ† ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù„Ø£Ù†Ù‡ Ù„Ø§ Ù†Ø¹Ø±Ù Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚Ø³Ø§Ø· Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©.)\n",
            "\n",
            "**Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ:** 8000 Ø±ÙŠØ§Ù„ - 5500 Ø±ÙŠØ§Ù„ = 2500 Ø±ÙŠØ§Ù„\n",
            "\n",
            "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§ØªØŒ **Ù„Ø§ ØªØ³ØªØ·ÙŠØ¹** Ø´Ø±Ø§Ø¡ Ø¢ÙŠÙÙˆÙ† Ø¨Ù€ 5000 Ø±ÙŠØ§Ù„ Ù„Ø£Ù†Ùƒ ØªÙ…ØªÙ„Ùƒ ÙÙ‚Ø· 2500 Ø±ÙŠØ§Ù„.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¦ INSTALL DEPENDENCIES\n",
        "!pip install -q google-generativeai gradio pandas scikit-learn\n",
        "\n",
        "# ğŸ“¥ IMPORTS\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "genai.configure(api_key=\"AIzaSyAeQdMgOlf33rJAhH3SojYfrg8es1N_37c\")\n",
        "\n",
        "# ğŸ§  LOAD DATASET\n",
        "df = pd.read_csv(\"/content/nabeeh_financial_data.csv\")\n",
        "\n",
        "# ğŸ·ï¸ CREATE TARGET VARIABLE\n",
        "df[\"TotalExpenses\"] = df[\"Essentials\"] + df[\"Lifestyle\"] + df[\"Loan\"] # Corrected column names\n",
        "df[\"Overspending\"] = (df[\"TotalExpenses\"] > 0.8 * df[\"Income\"]).astype(int) # Corrected column name\n",
        "\n",
        "# ğŸ¯ FEATURES & LABEL\n",
        "X = df[[\"Income\", \"Essentials\", \"Lifestyle\", \"Loan\"]] # Corrected column names\n",
        "y = df[\"Overspending\"]\n",
        "\n",
        "# ğŸ”€ SPLIT DATA\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ğŸ‹ï¸ TRAIN MODEL\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ğŸ¤– SET UP GEMINI\n",
        "gemini = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# ğŸ’¬ NABI'S RESPONSE LOGIC\n",
        "def nabeeh_advice(monthly_income, essential, discretionary, loan, question):\n",
        "    input_data = pd.DataFrame([[monthly_income, essential, discretionary, loan]],\n",
        "                              columns=[\"Income\", \"Essentials\", \"Lifestyle\", \"Loan\"]) # Corrected column names\n",
        "    prediction = model.predict(input_data)[0]\n",
        "\n",
        "    overspending_msg = \"ğŸ”´ Ø£Ù†Øª ØªØµØ±Ù Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ù„Ø§Ø²Ù…ØŒ Ø­Ø§ÙˆÙ„ ØªØ±Ø§Ø¬Ø¹ Ù…ØµØ§Ø±ÙŠÙÙƒ.\" if prediction else \"ğŸŸ¢ ÙˆØ¶Ø¹Ùƒ Ø§Ù„Ù…Ø§Ù„ÙŠ ØªØ­Øª Ø§Ù„Ø³ÙŠØ·Ø±Ø©ØŒ Ø§Ø³ØªÙ…Ø± ÙƒØ°Ø§.\"\n",
        "\n",
        "    gemini_prompt = f\"\"\"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n",
        "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª:\n",
        "- Ø§Ù„Ø¯Ø®Ù„ Ø§Ù„Ø´Ù‡Ø±ÙŠ: {monthly_income} Ø±ÙŠØ§Ù„\n",
        "- Ø§Ù„Ù…ØµØ§Ø±ÙŠÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {essential} Ø±ÙŠØ§Ù„\n",
        "- Ø§Ù„Ù…ØµØ§Ø±ÙŠÙ Ø§Ù„ØªØ±ÙÙŠÙ‡ÙŠØ©: {discretionary} Ø±ÙŠØ§Ù„\n",
        "- Ø§Ù„Ù‚Ø±ÙˆØ¶: {loan} Ø±ÙŠØ§Ù„\n",
        "- Ù‡Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØµØ±Ù ÙƒØ«ÙŠØ±ØŸ {'Ù†Ø¹Ù…' if prediction else 'Ù„Ø§'}\n",
        "\n",
        "Ø£Ø¹Ø·Ù†ÙŠ Ù†ØµÙŠØ­Ø© Ù…Ø§Ù„ÙŠØ© Ø´Ø®ØµÙŠØ© Ø°ÙƒÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.\"\"\"\n",
        "\n",
        "    try:\n",
        "        g_response = gemini.generate_content(gemini_prompt)\n",
        "        final_response = f\"{overspending_msg}\\n\\nğŸ§  Ù†Ø¨ÙŠÙ‡ ÙŠÙ‚ÙˆÙ„:\\n{g_response.text}\"\n",
        "    except Exception as e:\n",
        "        final_response = f\"{overspending_msg}\\n\\nâš ï¸ Ù†Ø¨ÙŠÙ‡ Ù…Ø§ Ù‚Ø¯Ø± ÙŠØ±Ø¯ Ø§Ù„Ø¢Ù†:\\n{str(e)}\"\n",
        "\n",
        "    return final_response\n",
        "\n",
        "# ğŸ¨ UI WITH GRADIO\n",
        "demo = gr.Interface(\n",
        "    fn=nabeeh_advice,\n",
        "    inputs=[\n",
        "        gr.Number(label=\"ğŸ’° Ø§Ù„Ø¯Ø®Ù„ Ø§Ù„Ø´Ù‡Ø±ÙŠ (Ø±ÙŠØ§Ù„)\"),\n",
        "        gr.Number(label=\"ğŸ’¸ Ø§Ù„Ù…ØµØ§Ø±ÙŠÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\"),\n",
        "        gr.Number(label=\"ğŸ‰ Ø§Ù„Ù…ØµØ§Ø±ÙŠÙ Ø§Ù„ØªØ±ÙÙŠÙ‡ÙŠØ©\"),\n",
        "        gr.Number(label=\"ğŸ¦ Ø¯ÙØ¹Ø§Øª Ø§Ù„Ù‚Ø±ÙˆØ¶\"),\n",
        "        gr.Textbox(label=\"ğŸ“ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ù…Ø§Ù„ÙŠ\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"ğŸ§  Ù†Ø¨ÙŠÙ‡\"),\n",
        "    title=\"Ù†Ø¨ÙŠÙ‡\",\n",
        "    description=\"Ø§Ø³Ø£Ù„ Ù†Ø¨ÙŠÙ‡ Ø¹Ù† ÙˆØ¶Ø¹Ùƒ Ø§Ù„Ù…Ø§Ù„ÙŠ ÙˆÙŠØ¹Ø·ÙŠÙƒ Ù†ØµÙŠØ­Ø© Ø°ÙƒÙŠØ© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\n",
        ")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "7hTmrcqXlKqp",
        "outputId": "ba3ed850-df79-4733-f3b5-5bfc794a28ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3842f529a057409618.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3842f529a057409618.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}